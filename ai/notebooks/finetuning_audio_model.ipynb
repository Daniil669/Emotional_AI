{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMsnDt__0Yzv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab69a240-5a64-4c4e-de4f-a71244eb4fba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade pip\n",
        "!pip -q install speechbrain==0.5.16 torch torchaudio==2.3.0 torchmetrics==1.4.0 numpy==1.26.4 scikit-learn==1.4.2 pandas==2.2.2 librosa==0.10.2.post1"
      ],
      "metadata": {
        "id": "328HiZhAdqvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean, runnable SER training cell (SpeechBrain + torchaudio)\n",
        "import os, json\n",
        "from pathlib import Path\n",
        "import torch, torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import speechbrain as sb\n",
        "from speechbrain.nnet.losses import nll_loss\n",
        "from torchmetrics.functional import f1_score, confusion_matrix, accuracy\n",
        "from torch.utils.data import DataLoader\n",
        "from speechbrain.dataio.dataset import DynamicItemDataset\n",
        "\n",
        "\n",
        "# Paths / labels\n",
        "BASE = Path(\"/content/drive/MyDrive/emotional_ai\")\n",
        "DATA_DIR = BASE / \"data\" / \"audio\" / \"ravdess\"\n",
        "AUDIO_ROOT = DATA_DIR / \"wav\"\n",
        "META_DIR = DATA_DIR / \"meta\"\n",
        "\n",
        "with open(META_DIR / \"labels.json\") as f:\n",
        "    LABELS = json.load(f)\n",
        "NUM_CLASSES = len(LABELS)\n",
        "label_to_index = {lab: i for i, lab in enumerate(LABELS)}\n",
        "\n",
        "train_csv = META_DIR / \"train.csv\"\n",
        "valid_csv = META_DIR / \"valid.csv\"\n",
        "test_csv  = META_DIR / \"test.csv\"\n",
        "assert train_csv.exists() and valid_csv.exists() and test_csv.exists(), \"Missing CSVs!\"\n",
        "\n",
        "# Dataset helpers\n",
        "SAMPLE_RATE = 16000\n",
        "\n",
        "def read_csv_to_items(path: Path):\n",
        "    df = pd.read_csv(path)  # expects: ID,wav,duration,emotion\n",
        "    df = df[df[\"emotion\"].isin(LABELS)]\n",
        "    items = []\n",
        "    for _, r in df.iterrows():\n",
        "        items.append({\n",
        "            \"id\": str(r[\"ID\"]),\n",
        "            \"wav\": str(r[\"wav\"]),\n",
        "            \"emotion\": label_to_index[str(r[\"emotion\"])],\n",
        "        })\n",
        "    return items\n",
        "\n",
        "def items_to_mapping(items):\n",
        "    # DynamicItemDataset expects dict keyed by \"id\"\n",
        "    return {it[\"id\"]: {\"wav\": it[\"wav\"], \"emotion\": it[\"emotion\"]} for it in items}\n",
        "\n",
        "train_items = read_csv_to_items(train_csv)\n",
        "valid_items = read_csv_to_items(valid_csv)\n",
        "test_items  = read_csv_to_items(test_csv)\n",
        "\n",
        "datasets = {\n",
        "    \"train\": DynamicItemDataset(items_to_mapping(train_items)),\n",
        "    \"valid\": DynamicItemDataset(items_to_mapping(valid_items)),\n",
        "    \"test\":  DynamicItemDataset(items_to_mapping(test_items)),\n",
        "}\n",
        "\n",
        "\n",
        "# Pipelines (audio + labels)\n",
        "def audio_pipeline(wav_path):\n",
        "    sig, sr = torchaudio.load(wav_path)      # [C, T]\n",
        "    if sig.shape[0] > 1:\n",
        "        sig = sig.mean(dim=0, keepdim=True)  # mono\n",
        "    if sr != SAMPLE_RATE:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=SAMPLE_RATE)\n",
        "        sig = resampler(sig)\n",
        "    return sig.squeeze(0)                     # [T]\n",
        "\n",
        "def label_pipeline(idx):\n",
        "    return torch.tensor(idx, dtype=torch.long)\n",
        "\n",
        "for split in datasets:\n",
        "    datasets[split].add_dynamic_item(audio_pipeline, takes=\"wav\", provides=\"sig\")\n",
        "    datasets[split].add_dynamic_item(label_pipeline,  takes=\"emotion\", provides=\"label\")\n",
        "    datasets[split].set_output_keys([\"id\", \"sig\", \"label\"])\n",
        "\n",
        "\n",
        "# Dataloaders (custom collate)\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Keep variable-length signals as a list; stack labels into a tensor\n",
        "    sigs = [b[\"sig\"] for b in batch]  # list of 1D tensors\n",
        "    labels = torch.tensor([int(b[\"label\"].item()) for b in batch], dtype=torch.long)\n",
        "    return {\"sig\": sigs, \"label\": labels}\n",
        "\n",
        "train_dataloader = DataLoader(datasets[\"train\"], batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn)\n",
        "valid_dataloader = DataLoader(datasets[\"valid\"], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_dataloader  = DataLoader(datasets[\"test\"],  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "# Features (log-Mel)\n",
        "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SAMPLE_RATE, n_fft=1024, hop_length=256, n_mels=80\n",
        ")\n",
        "ampl_to_db = torchaudio.transforms.AmplitudeToDB()\n",
        "\n",
        "def compute_features(sig_batch):\n",
        "    feats_list = []\n",
        "    for sig in sig_batch:                 # sig: 1D\n",
        "        S = mel_spec(sig)                 # [n_mels, T]\n",
        "        S_db = ampl_to_db(S).transpose(0, 1)  # [T, n_mels]\n",
        "        feats_list.append(S_db)\n",
        "    lens = torch.tensor([f.size(0) for f in feats_list], dtype=torch.long)\n",
        "    max_len = int(lens.max().item())\n",
        "    feat_dim = feats_list[0].size(1)\n",
        "    padded = torch.zeros(len(feats_list), max_len, feat_dim)\n",
        "    for i, f in enumerate(feats_list):\n",
        "        T = f.size(0)\n",
        "        padded[i, :T, :feat_dim] = f\n",
        "    return padded, lens\n",
        "\n",
        "# Model (CRDNN)\n",
        "class CRDNN(nn.Module):\n",
        "    def __init__(self, n_mels=80, cnn_channels=128, rnn_hidden=128, num_classes=NUM_CLASSES):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, cnn_channels, kernel_size=(5,5), stride=(1,1), padding=2),\n",
        "            nn.BatchNorm2d(cnn_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2,2)),\n",
        "            nn.Conv2d(cnn_channels, cnn_channels, kernel_size=(3,3), padding=1),\n",
        "            nn.BatchNorm2d(cnn_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2,2)),\n",
        "        )\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=(n_mels//4)*cnn_channels,\n",
        "            hidden_size=rnn_hidden,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_hidden*2, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, feats, lens):\n",
        "        # feats: [B, T, F] -> [B, 1, T, F]\n",
        "        x = feats.unsqueeze(1)\n",
        "        x = self.conv(x)           # [B, C, T', F']\n",
        "        B, C, Tprime, Fprime = x.shape\n",
        "        x = x.permute(0, 2, 1, 3).contiguous().view(B, Tprime, C*Fprime)\n",
        "        out, _ = self.rnn(x)\n",
        "        out = out.mean(dim=1)      # mean pool over time\n",
        "        logits = self.classifier(out)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Brain (with built-in accuracy counter)\n",
        "class SERBrain(sb.core.Brain):\n",
        "    def on_fit_start(self):\n",
        "        super().on_fit_start()\n",
        "        if \"optimizer\" not in self.checkpointer.recoverables:\n",
        "            self.checkpointer.add_recoverables({\n",
        "                \"model\": self.modules[\"model\"],\n",
        "                \"optimizer\": self.optimizer,\n",
        "                \"epoch_counter\": self.hparams.epoch_counter,\n",
        "            })\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        # batch is dict from the collate_fn\n",
        "        sigs = batch[\"sig\"]                               # list[Tensor(T_i)]\n",
        "        labels = batch[\"label\"].to(self.device)           # [B]\n",
        "        feats, lens = compute_features(sigs)              # -> [B, Tmax, F], [B]\n",
        "        feats = feats.to(self.device)\n",
        "        logits = self.modules.model(feats, lens)          # [B, C]\n",
        "        outputs = F.log_softmax(logits, dim=-1)\n",
        "        return outputs, labels\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        outputs, labels = predictions\n",
        "        loss = nll_loss(outputs, labels)\n",
        "\n",
        "        # Running accuracy for VALID/TEST (no external imports)\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            with torch.no_grad():\n",
        "                preds = outputs.argmax(dim=-1)\n",
        "                if not hasattr(self, \"_acc_correct\"):\n",
        "                    self._acc_correct, self._acc_total = 0, 0\n",
        "                self._acc_correct += (preds == labels).sum().item()\n",
        "                self._acc_total += labels.numel()\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self._acc_correct, self._acc_total = 0, 0\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        if stage == sb.Stage.VALID:\n",
        "            acc = (self._acc_correct / max(1, self._acc_total))\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch},\n",
        "                valid_stats={\"loss\": stage_loss, \"acc\": acc},\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(meta={\"acc\": acc}, min_keys=[\"acc\"])\n",
        "        elif stage == sb.Stage.TEST:\n",
        "            acc = (self._acc_correct / max(1, self._acc_total))\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats={\"loss\": stage_loss, \"acc\": acc},\n",
        "            )\n",
        "\n",
        "\n",
        "# Hyperparams / objects\n",
        "run_dir = BASE / \"runs\" / \"speechbrain_ravdess\"\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "hparams = {\n",
        "    \"lr\": 1e-3,\n",
        "    \"epochs\": 25,\n",
        "    \"opt_class\": torch.optim.Adam,\n",
        "    \"model\": CRDNN(n_mels=80, cnn_channels=128, rnn_hidden=128, num_classes=NUM_CLASSES),\n",
        "    \"train_logger\": sb.utils.train_logger.FileTrainLogger(str(run_dir / \"log.txt\")),\n",
        "    \"epoch_counter\": sb.utils.epoch_loop.EpochCounter(limit=25),\n",
        "    \"checkpointer\": sb.utils.checkpoints.Checkpointer(checkpoints_dir=str(run_dir / \"ckpt\")),\n",
        "}\n",
        "brain = SERBrain(\n",
        "    modules={\"model\": hparams[\"model\"]},\n",
        "    opt_class=hparams[\"opt_class\"],\n",
        "    hparams=hparams,\n",
        "    run_opts={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
        "    checkpointer=hparams[\"checkpointer\"]\n",
        ")\n",
        "\n",
        "\n",
        "# Train\n",
        "brain.fit(\n",
        "    epoch_counter=brain.hparams.epoch_counter,\n",
        "    train_set=train_dataloader,\n",
        "    valid_set=valid_dataloader,\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate (best ckpt) + metrics\n",
        "brain.checkpointer.recover_if_possible()\n",
        "\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        outputs, labels = brain.compute_forward(batch, sb.Stage.TEST)\n",
        "        _ = brain.compute_objectives((outputs, labels), batch, sb.Stage.TEST)\n",
        "        preds = outputs.argmax(dim=-1).cpu().numpy().tolist()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "all_preds_t = torch.tensor(all_preds)\n",
        "all_labels_t = torch.tensor(all_labels)\n",
        "test_acc = accuracy(all_preds_t, all_labels_t, task=\"multiclass\", num_classes=NUM_CLASSES).item()\n",
        "test_f1  = f1_score(all_preds_t, all_labels_t, task=\"multiclass\", num_classes=NUM_CLASSES, average=\"macro\").item()\n",
        "cm       = confusion_matrix(all_preds_t, all_labels_t, task=\"multiclass\", num_classes=NUM_CLASSES).cpu().numpy()\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test Macro-F1: {test_f1:.4f}\")\n",
        "print(\"Labels:\", LABELS)\n",
        "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
        "\n",
        "# Save artifacts\n",
        "np.save(run_dir / \"confusion_matrix.npy\", cm)\n",
        "with open(run_dir / \"labels.json\", \"w\") as f:\n",
        "    json.dump(LABELS, f, indent=2)\n",
        "torch.save(brain.modules[\"model\"].state_dict(), run_dir / \"model_best_state.pt\")\n",
        "\n",
        "# Robust TorchScript export (trace - fallback to script)\n",
        "import json, torch\n",
        "\n",
        "m = brain.modules[\"model\"].eval()\n",
        "\n",
        "# Put example tensors on the SAME device and dtype as the model\n",
        "device = next(m.parameters()).device\n",
        "ex_T = 400\n",
        "example_feats = torch.randn(1, ex_T, 80, device=device, dtype=torch.float32).contiguous()\n",
        "example_lens  = torch.tensor([ex_T], device=device, dtype=torch.long)\n",
        "\n",
        "with torch.inference_mode():\n",
        "    try:\n",
        "        # Try tracing first (fast, portable). strict=False tolerates control-flow it can’t follow.\n",
        "        ts = torch.jit.trace(m, (example_feats, example_lens), strict=False)\n",
        "        _ = ts(example_feats, example_lens)  # sanity run\n",
        "    except Exception as e:\n",
        "        # Fallback to scripting (handles more dynamic bits)\n",
        "        class Wrapper(torch.nn.Module):\n",
        "            def __init__(self, core):\n",
        "                super().__init__()\n",
        "                self.core = core\n",
        "            def forward(self, feats: torch.Tensor, lens: torch.Tensor):\n",
        "                return self.core(feats, lens)\n",
        "        wrapped = Wrapper(m).to(device).eval()\n",
        "        ts = torch.jit.script(wrapped)\n",
        "\n",
        "# Save TorchScript on CPU for portability\n",
        "ts = ts.to(\"cpu\")\n",
        "ts_path = run_dir / \"model_best_ts.pt\"\n",
        "ts.save(str(ts_path))\n",
        "\n",
        "# Save labels next to the model\n",
        "with open(run_dir / \"labels.json\", \"w\") as f:\n",
        "    json.dump(LABELS, f, indent=2)\n",
        "\n",
        "print(\"Saved:\", ts_path)\n",
        "print(\"Labels:\", LABELS)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7W5nyshyfZBV",
        "outputId": "4d9159fc-e0f8-4e38-b6b7-e50c5ba641ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 87/87 [00:38<00:00,  2.24it/s, train_loss=1.9]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.65it/s]\n",
            "100%|██████████| 87/87 [00:39<00:00,  2.19it/s, train_loss=1.79]\n",
            "100%|██████████| 18/18 [00:07<00:00,  2.38it/s]\n",
            "100%|██████████| 87/87 [00:38<00:00,  2.23it/s, train_loss=1.78]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.82it/s]\n",
            "100%|██████████| 87/87 [00:39<00:00,  2.22it/s, train_loss=1.73]\n",
            "100%|██████████| 18/18 [00:07<00:00,  2.34it/s]\n",
            "100%|██████████| 87/87 [00:39<00:00,  2.21it/s, train_loss=1.68]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.78it/s]\n",
            "100%|██████████| 87/87 [00:39<00:00,  2.21it/s, train_loss=1.63]\n",
            "100%|██████████| 18/18 [00:07<00:00,  2.36it/s]\n",
            "100%|██████████| 87/87 [00:39<00:00,  2.23it/s, train_loss=1.59]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.73it/s]\n",
            "100%|██████████| 87/87 [00:39<00:00,  2.20it/s, train_loss=1.51]\n",
            "100%|██████████| 18/18 [00:07<00:00,  2.38it/s]\n",
            "100%|██████████| 87/87 [00:39<00:00,  2.18it/s, train_loss=1.47]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.69it/s]\n",
            "100%|██████████| 87/87 [00:39<00:00,  2.22it/s, train_loss=1.41]\n",
            "100%|██████████| 18/18 [00:07<00:00,  2.48it/s]\n",
            "100%|██████████| 87/87 [00:39<00:00,  2.19it/s, train_loss=1.35]\n",
            "100%|██████████| 18/18 [00:07<00:00,  2.46it/s]\n",
            "100%|██████████| 87/87 [00:39<00:00,  2.20it/s, train_loss=1.29]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.62it/s]\n",
            "100%|██████████| 87/87 [00:40<00:00,  2.17it/s, train_loss=1.27]\n",
            "100%|██████████| 18/18 [00:07<00:00,  2.37it/s]\n",
            "100%|██████████| 87/87 [00:39<00:00,  2.20it/s, train_loss=1.2]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.72it/s]\n",
            "100%|██████████| 87/87 [00:39<00:00,  2.19it/s, train_loss=1.16]\n",
            "100%|██████████| 18/18 [00:07<00:00,  2.41it/s]\n",
            "100%|██████████| 87/87 [00:39<00:00,  2.18it/s, train_loss=1.06]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.79it/s]\n",
            "100%|██████████| 87/87 [00:39<00:00,  2.19it/s, train_loss=1.07]\n",
            "100%|██████████| 18/18 [00:07<00:00,  2.45it/s]\n",
            "100%|██████████| 87/87 [00:40<00:00,  2.17it/s, train_loss=1.01]\n",
            "100%|██████████| 18/18 [00:07<00:00,  2.42it/s]\n",
            "100%|██████████| 87/87 [00:39<00:00,  2.18it/s, train_loss=0.992]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.79it/s]\n",
            "100%|██████████| 87/87 [00:39<00:00,  2.20it/s, train_loss=0.89]\n",
            "100%|██████████| 18/18 [00:07<00:00,  2.32it/s]\n",
            "100%|██████████| 87/87 [00:39<00:00,  2.19it/s, train_loss=0.903]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.69it/s]\n",
            "100%|██████████| 87/87 [00:39<00:00,  2.19it/s, train_loss=0.787]\n",
            "100%|██████████| 18/18 [00:07<00:00,  2.42it/s]\n",
            "100%|██████████| 87/87 [00:40<00:00,  2.17it/s, train_loss=0.745]\n",
            "100%|██████████| 18/18 [00:08<00:00,  2.23it/s]\n",
            "100%|██████████| 87/87 [00:40<00:00,  2.14it/s, train_loss=0.74]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.59it/s]\n",
            "100%|██████████| 87/87 [00:40<00:00,  2.16it/s, train_loss=0.685]\n",
            "100%|██████████| 18/18 [00:07<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7593\n",
            "Test Macro-F1: 0.7526\n",
            "Labels: ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
            "Confusion matrix (rows=true, cols=pred):\n",
            " [[30  0  4  1  0  0  1  0]\n",
            " [ 0 34  0  0  0  0  0  0]\n",
            " [ 1  0 31  0  0  0  1  0]\n",
            " [ 0  1  0 25  2  0  8  0]\n",
            " [ 0  1  0  1 24  5  7  0]\n",
            " [ 0  8  0  0  0 11  0  0]\n",
            " [ 0 15  0  3  3  0 16  0]\n",
            " [ 0  0  1  1  1  0  0 34]]\n",
            "Saved: /content/drive/MyDrive/emotional_ai/runs/speechbrain_ravdess/model_best_ts.pt\n",
            "Labels: ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n"
          ]
        }
      ]
    }
  ]
}