{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Is7JgDAhMOf",
        "outputId": "4bc03e4b-28ce-4a06-93c9-95d7f9531c5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n"
      ],
      "metadata": {
        "id": "K1bg_whtqbad"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip -q install -U pip\n",
        "\n",
        "\n",
        "!pip -q uninstall -y numpy pandas scipy torchvision fastai fastdownload timm \\\n",
        "  opencv-python opencv-python-headless opencv-contrib-python cuml-cu12 umap-learn || true\n",
        "\n",
        "\n",
        "!pip -q install torch==2.3.0 torchaudio==2.3.0 --extra-index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "\n",
        "!pip -q install numpy==2.0.1 pandas==2.2.2 scipy==1.14.1\n",
        "\n",
        "\n",
        "!pip -q install librosa==0.10.2.post1 speechbrain==0.5.16 torchmetrics==1.4.0\n",
        "\n",
        "\n",
        "import os, time; print(\"Restarting runtime...\"); time.sleep(0.5); os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oARwt1jwhvRi",
        "outputId": "86dbf169-4235-4c06-fa7f-b5bed29f776b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping fastai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping fastdownload as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping timm as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping opencv-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping opencv-python-headless as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping opencv-contrib-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping cuml-cu12 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping umap-learn as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires opencv-python>=3.4.8.29, which is not installed.\n",
            "albumentations 2.0.8 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
            "albucore 0.0.24 requires opencv-python-headless>=4.9.0.80, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mRestarting runtime...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, numpy, pandas, torch, torchaudio, scipy, librosa\n",
        "print(\"py\", sys.version)\n",
        "print(\"numpy\", numpy.__version__)\n",
        "print(\"pandas\", pandas.__version__)\n",
        "print(\"scipy\", scipy.__version__)\n",
        "print(\"torch\", torch.__version__, \"torchaudio\", torchaudio.__version__)\n",
        "print(\"librosa\", librosa.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrOhmQJ1sg_C",
        "outputId": "e4aa8a5b-53b9-4596-d765-06a61562df7a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "py 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
            "numpy 2.0.1\n",
            "pandas 2.2.2\n",
            "scipy 1.14.1\n",
            "torch 2.3.0+cu121 torchaudio 2.3.0+cu121\n",
            "librosa 0.10.2.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deterministic CRDNN training on RAVDESS (GPU)\n",
        "import os, json, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch, torchaudio, torch.nn as nn, torch.nn.functional as F\n",
        "import pandas as pd\n",
        "\n",
        "import speechbrain as sb\n",
        "from speechbrain.nnet.losses import nll_loss\n",
        "from torchmetrics.functional import f1_score, confusion_matrix, accuracy\n",
        "from torch.utils.data import DataLoader\n",
        "from speechbrain.dataio.dataset import DynamicItemDataset\n",
        "\n",
        "# Reproducibility\n",
        "def set_seed(s=42):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(s)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "set_seed(42)\n",
        "\n",
        "# Paths / labels\n",
        "BASE = Path(\"/content/drive/MyDrive/emotional_ai\")\n",
        "DATA_DIR = BASE / \"data\" / \"audio\" / \"ravdess\"\n",
        "AUDIO_ROOT = DATA_DIR / \"wav\"\n",
        "META_DIR = DATA_DIR / \"meta\"\n",
        "\n",
        "with open(META_DIR / \"labels.json\") as f:\n",
        "    LABELS = json.load(f)\n",
        "NUM_CLASSES = len(LABELS)\n",
        "label_to_index = {lab: i for i, lab in enumerate(LABELS)}\n",
        "\n",
        "train_csv = META_DIR / \"train.csv\"\n",
        "valid_csv = META_DIR / \"valid.csv\"\n",
        "test_csv  = META_DIR / \"test.csv\"\n",
        "assert train_csv.exists() and valid_csv.exists() and test_csv.exists(), \"Missing CSVs!\"\n",
        "\n",
        "# Dataset helpers\n",
        "SAMPLE_RATE = 16000\n",
        "\n",
        "def read_csv_to_items(path: Path):\n",
        "    df = pd.read_csv(path)# expects: ID,wav,duration,emotion\n",
        "    df = df[df[\"emotion\"].isin(LABELS)]\n",
        "    items = []\n",
        "    for _, r in df.iterrows():\n",
        "        items.append({\n",
        "            \"id\": str(r[\"ID\"]),\n",
        "            \"wav\": str(r[\"wav\"]),\n",
        "            \"emotion\": label_to_index[str(r[\"emotion\"])],\n",
        "        })\n",
        "    return items\n",
        "\n",
        "def items_to_mapping(items):\n",
        "    return {it[\"id\"]: {\"wav\": it[\"wav\"], \"emotion\": it[\"emotion\"]} for it in items}\n",
        "\n",
        "train_items = read_csv_to_items(train_csv)\n",
        "valid_items = read_csv_to_items(valid_csv)\n",
        "test_items  = read_csv_to_items(test_csv)\n",
        "\n",
        "datasets = {\n",
        "    \"train\": DynamicItemDataset(items_to_mapping(train_items)),\n",
        "    \"valid\": DynamicItemDataset(items_to_mapping(valid_items)),\n",
        "    \"test\":  DynamicItemDataset(items_to_mapping(test_items)),\n",
        "}\n",
        "\n",
        "# Pipelines (audio and labels)\n",
        "def audio_pipeline(wav_path):\n",
        "    sig, sr = torchaudio.load(wav_path)# [C, T]\n",
        "    if sig.shape[0] > 1:\n",
        "        sig = sig.mean(dim=0, keepdim=True)# mono\n",
        "    if sr != SAMPLE_RATE:\n",
        "        sig = torchaudio.transforms.Resample(orig_freq=sr, new_freq=SAMPLE_RATE)(sig)\n",
        "    return sig.squeeze(0)# [T]\n",
        "\n",
        "def label_pipeline(idx):\n",
        "    return torch.tensor(idx, dtype=torch.long)\n",
        "\n",
        "for split in datasets:\n",
        "    datasets[split].add_dynamic_item(audio_pipeline, takes=\"wav\", provides=\"sig\")\n",
        "    datasets[split].add_dynamic_item(label_pipeline,  takes=\"emotion\", provides=\"label\")\n",
        "    datasets[split].set_output_keys([\"id\", \"sig\", \"label\"])\n",
        "\n",
        "# Dataloaders\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "def collate_fn(batch):\n",
        "    sigs = [b[\"sig\"] for b in batch]\n",
        "    labels = torch.tensor([int(b[\"label\"].item()) for b in batch], dtype=torch.long)\n",
        "    return {\"sig\": sigs, \"label\": labels}\n",
        "\n",
        "train_dataloader = DataLoader(datasets[\"train\"], batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn)\n",
        "valid_dataloader = DataLoader(datasets[\"valid\"], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_dataloader  = DataLoader(datasets[\"test\"],  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Features (log-Mel)\n",
        "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SAMPLE_RATE, n_fft=1024, hop_length=256, n_mels=80\n",
        ")\n",
        "ampl_to_db = torchaudio.transforms.AmplitudeToDB()\n",
        "\n",
        "def compute_features(sig_batch):\n",
        "    feats_list = []\n",
        "    for sig in sig_batch:\n",
        "        S = mel_spec(sig)# [n_mels, T]\n",
        "        S_db = ampl_to_db(S).transpose(0, 1)# [T, n_mels]\n",
        "        feats_list.append(S_db)\n",
        "    lens = torch.tensor([f.size(0) for f in feats_list], dtype=torch.long)\n",
        "    max_len = int(lens.max().item())\n",
        "    feat_dim = feats_list[0].size(1)\n",
        "    padded = torch.zeros(len(feats_list), max_len, feat_dim)\n",
        "    for i, f in enumerate(feats_list):\n",
        "        T = f.size(0)\n",
        "        padded[i, :T, :feat_dim] = f\n",
        "    return padded, lens\n",
        "\n",
        "# Model (CRDNN)\n",
        "class CRDNN(nn.Module):\n",
        "    def __init__(self, n_mels=80, cnn_channels=128, rnn_hidden=128, num_classes=NUM_CLASSES):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, cnn_channels, kernel_size=(5,5), stride=(1,1), padding=2),\n",
        "            nn.BatchNorm2d(cnn_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2,2)),\n",
        "            nn.Conv2d(cnn_channels, cnn_channels, kernel_size=(3,3), padding=1),\n",
        "            nn.BatchNorm2d(cnn_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2,2)),\n",
        "        )\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=(n_mels//4)*cnn_channels,\n",
        "            hidden_size=128,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128*2, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, feats, lens):\n",
        "        x = feats.unsqueeze(1)# [B, 1, T, F]\n",
        "        x = self.conv(x)# [B, C, T', F']\n",
        "        B, C, Tprime, Fprime = x.shape\n",
        "        x = x.permute(0, 2, 1, 3).contiguous().view(B, Tprime, C*Fprime)\n",
        "        out, _ = self.rnn(x)\n",
        "        out = out.mean(dim=1)# mean over time\n",
        "        return self.classifier(out)\n",
        "\n",
        "# Brain\n",
        "class SERBrain(sb.core.Brain):\n",
        "    def on_fit_start(self):\n",
        "        super().on_fit_start()\n",
        "        if \"optimizer\" not in self.checkpointer.recoverables:\n",
        "            self.checkpointer.add_recoverables({\n",
        "                \"model\": self.modules[\"model\"],\n",
        "                \"optimizer\": self.optimizer,\n",
        "                \"epoch_counter\": self.hparams.epoch_counter,\n",
        "            })\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        sigs = batch[\"sig\"]\n",
        "        labels = batch[\"label\"].to(self.device)\n",
        "        feats, lens = compute_features(sigs)\n",
        "        feats = feats.to(self.device)\n",
        "        logits = self.modules.model(feats, lens)\n",
        "        outputs = F.log_softmax(logits, dim=-1)\n",
        "        return outputs, labels\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        outputs, labels = predictions\n",
        "        loss = nll_loss(outputs, labels)\n",
        "        if stage != sb.Stage.TRAIN:  # running accuracy\n",
        "            with torch.no_grad():\n",
        "                preds = outputs.argmax(dim=-1)\n",
        "                if not hasattr(self, \"_acc_correct\"):\n",
        "                    self._acc_correct, self._acc_total = 0, 0\n",
        "                self._acc_correct += (preds == labels).sum().item()\n",
        "                self._acc_total += labels.numel()\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self._acc_correct, self._acc_total = 0, 0\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        if stage == sb.Stage.VALID:\n",
        "            acc = (self._acc_correct / max(1, self._acc_total))\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch},\n",
        "                valid_stats={\"loss\": stage_loss, \"acc\": acc},\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(meta={\"acc\": acc}, min_keys=[\"acc\"])\n",
        "        elif stage == sb.Stage.TEST:\n",
        "            acc = (self._acc_correct / max(1, self._acc_total))\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats={\"loss\": stage_loss, \"acc\": acc},\n",
        "            )\n",
        "\n",
        "# Hyperparams / objects\n",
        "run_dir = BASE / \"runs\" / \"speechbrain_ravdess\"\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "hparams = {\n",
        "    \"lr\": 1e-3,\n",
        "    \"epochs\": 25,\n",
        "    \"opt_class\": torch.optim.Adam,\n",
        "    \"model\": CRDNN(n_mels=80, cnn_channels=128, rnn_hidden=128, num_classes=NUM_CLASSES),\n",
        "    \"train_logger\": sb.utils.train_logger.FileTrainLogger(str(run_dir / \"log.txt\")),\n",
        "    \"epoch_counter\": sb.utils.epoch_loop.EpochCounter(limit=25),\n",
        "    \"checkpointer\": sb.utils.checkpoints.Checkpointer(checkpoints_dir=str(run_dir / \"ckpt\")),\n",
        "}\n",
        "brain = SERBrain(\n",
        "    modules={\"model\": hparams[\"model\"]},\n",
        "    opt_class=hparams[\"opt_class\"],\n",
        "    hparams=hparams,\n",
        "    run_opts={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
        "    checkpointer=hparams[\"checkpointer\"]\n",
        ")\n",
        "\n",
        "# Train\n",
        "brain.fit(\n",
        "    epoch_counter=brain.hparams.epoch_counter,\n",
        "    train_set=train_dataloader,\n",
        "    valid_set=valid_dataloader,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baWpbDJOh25N",
        "outputId": "ec3fd69d-8f1c-4d1c-a8f3-011b5121741b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:speechbrain.utils.train_logger:torchvision is not available - cannot save figures\n",
            "100%|██████████| 87/87 [07:32<00:00,  5.20s/it, train_loss=1.9]\n",
            "100%|██████████| 18/18 [01:14<00:00,  4.16s/it]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.42it/s, train_loss=1.79]\n",
            "100%|██████████| 18/18 [00:07<00:00,  2.57it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.37it/s, train_loss=1.66]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.94it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.35it/s, train_loss=1.58]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.83it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.38it/s, train_loss=1.46]\n",
            "100%|██████████| 18/18 [00:07<00:00,  2.51it/s]\n",
            "100%|██████████| 87/87 [00:37<00:00,  2.34it/s, train_loss=1.33]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.92it/s]\n",
            "100%|██████████| 87/87 [00:37<00:00,  2.34it/s, train_loss=1.22]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.93it/s]\n",
            "100%|██████████| 87/87 [00:37<00:00,  2.35it/s, train_loss=1.12]\n",
            "100%|██████████| 18/18 [00:07<00:00,  2.51it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.41it/s, train_loss=1]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.73it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.37it/s, train_loss=0.966]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.96it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.35it/s, train_loss=0.912]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.61it/s]\n",
            "100%|██████████| 87/87 [00:35<00:00,  2.44it/s, train_loss=0.784]\n",
            "100%|██████████| 18/18 [00:07<00:00,  2.55it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.37it/s, train_loss=0.707]\n",
            "100%|██████████| 18/18 [00:05<00:00,  3.00it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.36it/s, train_loss=0.687]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.87it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.39it/s, train_loss=0.59]\n",
            "100%|██████████| 18/18 [00:07<00:00,  2.54it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.38it/s, train_loss=0.539]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.88it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.37it/s, train_loss=0.476]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.96it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.35it/s, train_loss=0.472]\n",
            "100%|██████████| 18/18 [00:07<00:00,  2.54it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.39it/s, train_loss=0.325]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.66it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.38it/s, train_loss=0.348]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.98it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.36it/s, train_loss=0.258]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.59it/s]\n",
            "100%|██████████| 87/87 [00:35<00:00,  2.43it/s, train_loss=0.265]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.59it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.37it/s, train_loss=0.169]\n",
            "100%|██████████| 18/18 [00:06<00:00,  3.00it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.37it/s, train_loss=0.183]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.72it/s]\n",
            "100%|██████████| 87/87 [00:35<00:00,  2.44it/s, train_loss=0.26]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.58it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate (best ckpt)\n",
        "brain.checkpointer.recover_if_possible()\n",
        "\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        outputs, labels = brain.compute_forward(batch, sb.Stage.TEST)\n",
        "        preds = outputs.argmax(dim=-1).cpu().numpy().tolist()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "all_preds_t = torch.tensor(all_preds)\n",
        "all_labels_t = torch.tensor(all_labels)\n",
        "test_acc = accuracy(all_preds_t, all_labels_t, task=\"multiclass\", num_classes=len(LABELS)).item()\n",
        "test_f1  = f1_score(all_preds_t, all_labels_t, task=\"multiclass\", num_classes=len(LABELS), average=\"macro\").item()\n",
        "cm       = confusion_matrix(all_preds_t, all_labels_t, task=\"multiclass\", num_classes=len(LABELS)).cpu().numpy()\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test Macro-F1: {test_f1:.4f}\")\n",
        "print(\"Labels:\", LABELS)\n",
        "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
        "\n",
        "# Save artifacts\n",
        "np.save(run_dir / \"confusion_matrix.npy\", cm)\n",
        "with open(run_dir / \"labels.json\", \"w\") as f:\n",
        "    json.dump(LABELS, f, indent=2)\n",
        "torch.save(brain.modules[\"model\"].state_dict(), run_dir / \"model_best_state.pt\")\n",
        "\n",
        "# TorchScript export (trace → fallback to script)\n",
        "m = brain.modules[\"model\"].eval()\n",
        "device = next(m.parameters()).device\n",
        "ex_T = 400\n",
        "example_feats = torch.randn(1, ex_T, 80, device=device, dtype=torch.float32).contiguous()\n",
        "example_lens  = torch.tensor([ex_T], device=device, dtype=torch.long)\n",
        "\n",
        "with torch.inference_mode():\n",
        "    try:\n",
        "        ts = torch.jit.trace(m, (example_feats, example_lens), strict=False)\n",
        "        _ = ts(example_feats, example_lens)\n",
        "    except Exception:\n",
        "        class Wrapper(torch.nn.Module):\n",
        "            def __init__(self, core): super().__init__(); self.core = core\n",
        "            def forward(self, feats: torch.Tensor, lens: torch.Tensor): return self.core(feats, lens)\n",
        "        ts = torch.jit.script(Wrapper(m).to(device).eval())\n",
        "\n",
        "ts = ts.to(\"cpu\")\n",
        "ts_path = run_dir / \"model_best_ts.pt\"\n",
        "ts.save(str(ts_path))\n",
        "print(\"Saved TorchScript to:\", ts_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4eizvjcoWLg",
        "outputId": "c25c5062-02ea-479f-c190-01afbfe9bd10"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9111\n",
            "Test Macro-F1: 0.9120\n",
            "Labels: ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
            "Confusion matrix (rows=true, cols=pred):\n",
            " [[36  0  0  0  0  0  0  0]\n",
            " [ 0 33  0  0  0  0  1  0]\n",
            " [ 3  0 30  0  0  0  0  0]\n",
            " [ 0  0  0 30  2  0  1  3]\n",
            " [ 1  0  0  1 30  1  1  4]\n",
            " [ 0  0  0  0  0 17  2  0]\n",
            " [ 1  0  2  0  0  0 34  0]\n",
            " [ 0  0  0  1  0  0  0 36]]\n",
            "Saved TorchScript to: /content/drive/MyDrive/emotional_ai/runs/speechbrain_ravdess/model_best_ts.pt\n"
          ]
        }
      ]
    }
  ]
}