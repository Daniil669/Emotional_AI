{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Is7JgDAhMOf",
        "outputId": "8f05b149-369d-4ff7-f298-31dfd1b68787"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1bg_whtqbad"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oARwt1jwhvRi",
        "outputId": "8b7c8481-2959-45fe-9e60-11b949518d8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.8 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 25.6.0 requires numpy<3.0a0,>=1.23, which is not installed.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, which is not installed.\n",
            "sentence-transformers 5.1.0 requires scipy, which is not installed.\n",
            "dask-cudf-cu12 25.6.0 requires numpy<3.0a0,>=1.23, which is not installed.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, which is not installed.\n",
            "peft 0.17.1 requires numpy>=1.17, which is not installed.\n",
            "pylibraft-cu12 25.6.0 requires numpy<3.0a0,>=1.23, which is not installed.\n",
            "accelerate 1.10.1 requires numpy<3.0.0,>=1.17, which is not installed.\n",
            "torchtune 0.6.1 requires numpy, which is not installed.\n",
            "xgboost 3.0.4 requires numpy, which is not installed.\n",
            "xgboost 3.0.4 requires scipy, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.24 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
            "albumentations 2.0.8 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
            "dopamine-rl 4.1.2 requires opencv-python>=3.4.8.29, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip -q install -U pip\n",
        "\n",
        "\n",
        "!pip -q uninstall -y numpy pandas scipy torchvision fastai fastdownload timm \\\n",
        "  opencv-python opencv-python-headless opencv-contrib-python cuml-cu12 umap-learn || true\n",
        "\n",
        "\n",
        "!pip -q install torch==2.3.0 torchaudio==2.3.0 --extra-index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "\n",
        "!pip -q install numpy==2.0.1 pandas==2.2.2 scipy==1.14.1\n",
        "\n",
        "\n",
        "!pip -q install librosa==0.10.2.post1 speechbrain==0.5.16 torchmetrics==1.4.0\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EOLxpr9kyEy",
        "outputId": "229b2f5f-095b-40dd-9c9e-285495f1b37d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Restarting runtime...\n"
          ]
        }
      ],
      "source": [
        "import os, time; print(\"Restarting runtime...\"); time.sleep(0.5); os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrOhmQJ1sg_C",
        "outputId": "e630bb3e-8633-4477-99db-0b938d5a6ea1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "py 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
            "numpy 2.0.2\n",
            "pandas 2.2.2\n",
            "scipy 1.16.1\n",
            "torch 2.8.0+cu126 torchaudio 2.8.0+cu126\n",
            "librosa 0.11.0\n"
          ]
        }
      ],
      "source": [
        "import sys, numpy, pandas, torch, torchaudio, scipy, librosa\n",
        "print(\"py\", sys.version)\n",
        "print(\"numpy\", numpy.__version__)\n",
        "print(\"pandas\", pandas.__version__)\n",
        "print(\"scipy\", scipy.__version__)\n",
        "print(\"torch\", torch.__version__, \"torchaudio\", torchaudio.__version__)\n",
        "print(\"librosa\", librosa.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baWpbDJOh25N",
        "outputId": "3b25fe9c-1b64-4a20-acbd-8bbac04592a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:speechbrain.utils.train_logger:torchvision is not available - cannot save figures\n",
            "100%|██████████| 87/87 [11:14<00:00,  7.76s/it, train_loss=1.92]\n",
            "100%|██████████| 18/18 [02:32<00:00,  8.50s/it]\n",
            "100%|██████████| 87/87 [00:35<00:00,  2.43it/s, train_loss=1.86]\n",
            "100%|██████████| 18/18 [00:05<00:00,  3.09it/s]\n",
            "100%|██████████| 87/87 [00:37<00:00,  2.35it/s, train_loss=1.74]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.87it/s]\n",
            "100%|██████████| 87/87 [00:35<00:00,  2.42it/s, train_loss=1.71]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.60it/s]\n",
            "100%|██████████| 87/87 [00:35<00:00,  2.46it/s, train_loss=1.61]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.61it/s]\n",
            "100%|██████████| 87/87 [00:35<00:00,  2.42it/s, train_loss=1.54]\n",
            "100%|██████████| 18/18 [00:05<00:00,  3.05it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.41it/s, train_loss=1.46]\n",
            "100%|██████████| 18/18 [00:05<00:00,  3.03it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.42it/s, train_loss=1.36]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.81it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.39it/s, train_loss=1.29]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.60it/s]\n",
            "100%|██████████| 87/87 [00:35<00:00,  2.45it/s, train_loss=1.21]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.82it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.41it/s, train_loss=1.17]\n",
            "100%|██████████| 18/18 [00:05<00:00,  3.06it/s]\n",
            "100%|██████████| 87/87 [00:35<00:00,  2.42it/s, train_loss=1.05]\n",
            "100%|██████████| 18/18 [00:05<00:00,  3.05it/s]\n",
            "100%|██████████| 87/87 [00:35<00:00,  2.44it/s, train_loss=0.972]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.59it/s]\n",
            "100%|██████████| 87/87 [00:35<00:00,  2.47it/s, train_loss=0.975]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.60it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.36it/s, train_loss=0.847]\n",
            "100%|██████████| 18/18 [00:05<00:00,  3.02it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.41it/s, train_loss=0.827]\n",
            "100%|██████████| 18/18 [00:05<00:00,  3.15it/s]\n",
            "100%|██████████| 87/87 [00:35<00:00,  2.46it/s, train_loss=0.785]\n",
            "100%|██████████| 18/18 [00:05<00:00,  3.01it/s]\n",
            "100%|██████████| 87/87 [00:35<00:00,  2.43it/s, train_loss=0.709]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.60it/s]\n",
            "100%|██████████| 87/87 [00:35<00:00,  2.46it/s, train_loss=0.638]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.63it/s]\n",
            "100%|██████████| 87/87 [00:35<00:00,  2.43it/s, train_loss=0.577]\n",
            "100%|██████████| 18/18 [00:05<00:00,  3.06it/s]\n",
            "100%|██████████| 87/87 [00:37<00:00,  2.35it/s, train_loss=0.474]\n",
            "100%|██████████| 18/18 [00:05<00:00,  3.04it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.40it/s, train_loss=0.506]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.66it/s]\n",
            "100%|██████████| 87/87 [00:35<00:00,  2.47it/s, train_loss=0.412]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.60it/s]\n",
            "100%|██████████| 87/87 [00:35<00:00,  2.42it/s, train_loss=0.381]\n",
            "100%|██████████| 18/18 [00:06<00:00,  2.90it/s]\n",
            "100%|██████████| 87/87 [00:36<00:00,  2.41it/s, train_loss=0.427]\n",
            "100%|██████████| 18/18 [00:05<00:00,  3.03it/s]\n"
          ]
        }
      ],
      "source": [
        "# Deterministic CRDNN training on RAVDESS (GPU)\n",
        "import os, json, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch, torchaudio, torch.nn as nn, torch.nn.functional as F\n",
        "import pandas as pd\n",
        "\n",
        "import speechbrain as sb\n",
        "from speechbrain.nnet.losses import nll_loss\n",
        "from torchmetrics.functional import f1_score, confusion_matrix, accuracy\n",
        "from torch.utils.data import DataLoader\n",
        "from speechbrain.dataio.dataset import DynamicItemDataset\n",
        "\n",
        "# Reproducibility\n",
        "def set_seed(s=42):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(s)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "set_seed(42)\n",
        "\n",
        "# Paths / labels\n",
        "BASE = Path(\"/content/drive/MyDrive/emotional_ai\")\n",
        "DATA_DIR = BASE / \"data\" / \"audio\" / \"ravdess\"\n",
        "AUDIO_ROOT = DATA_DIR / \"wav\"\n",
        "META_DIR = DATA_DIR / \"meta\"\n",
        "\n",
        "with open(META_DIR / \"labels.json\") as f:\n",
        "    LABELS = json.load(f)\n",
        "NUM_CLASSES = len(LABELS)\n",
        "label_to_index = {lab: i for i, lab in enumerate(LABELS)}\n",
        "\n",
        "train_csv = META_DIR / \"train.csv\"\n",
        "valid_csv = META_DIR / \"valid.csv\"\n",
        "test_csv  = META_DIR / \"test.csv\"\n",
        "assert train_csv.exists() and valid_csv.exists() and test_csv.exists(), \"Missing CSVs!\"\n",
        "\n",
        "# Dataset helpers\n",
        "SAMPLE_RATE = 16000\n",
        "\n",
        "def read_csv_to_items(path: Path):\n",
        "    df = pd.read_csv(path)# expects: ID,wav,duration,emotion\n",
        "    df = df[df[\"emotion\"].isin(LABELS)]\n",
        "    items = []\n",
        "    for _, r in df.iterrows():\n",
        "        items.append({\n",
        "            \"id\": str(r[\"ID\"]),\n",
        "            \"wav\": str(r[\"wav\"]),\n",
        "            \"emotion\": label_to_index[str(r[\"emotion\"])],\n",
        "        })\n",
        "    return items\n",
        "\n",
        "def items_to_mapping(items):\n",
        "    return {it[\"id\"]: {\"wav\": it[\"wav\"], \"emotion\": it[\"emotion\"]} for it in items}\n",
        "\n",
        "train_items = read_csv_to_items(train_csv)\n",
        "valid_items = read_csv_to_items(valid_csv)\n",
        "test_items  = read_csv_to_items(test_csv)\n",
        "\n",
        "datasets = {\n",
        "    \"train\": DynamicItemDataset(items_to_mapping(train_items)),\n",
        "    \"valid\": DynamicItemDataset(items_to_mapping(valid_items)),\n",
        "    \"test\":  DynamicItemDataset(items_to_mapping(test_items)),\n",
        "}\n",
        "\n",
        "# Pipelines (audio and labels)\n",
        "def audio_pipeline(wav_path):\n",
        "    sig, sr = torchaudio.load(wav_path)# [C, T]\n",
        "    if sig.shape[0] > 1:\n",
        "        sig = sig.mean(dim=0, keepdim=True)# mono\n",
        "    if sr != SAMPLE_RATE:\n",
        "        sig = torchaudio.transforms.Resample(orig_freq=sr, new_freq=SAMPLE_RATE)(sig)\n",
        "    return sig.squeeze(0)# [T]\n",
        "\n",
        "def label_pipeline(idx):\n",
        "    return torch.tensor(idx, dtype=torch.long)\n",
        "\n",
        "for split in datasets:\n",
        "    datasets[split].add_dynamic_item(audio_pipeline, takes=\"wav\", provides=\"sig\")\n",
        "    datasets[split].add_dynamic_item(label_pipeline,  takes=\"emotion\", provides=\"label\")\n",
        "    datasets[split].set_output_keys([\"id\", \"sig\", \"label\"])\n",
        "\n",
        "# Dataloaders\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "def collate_fn(batch):\n",
        "    sigs = [b[\"sig\"] for b in batch]\n",
        "    labels = torch.tensor([int(b[\"label\"].item()) for b in batch], dtype=torch.long)\n",
        "    return {\"sig\": sigs, \"label\": labels}\n",
        "\n",
        "train_dataloader = DataLoader(datasets[\"train\"], batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn)\n",
        "valid_dataloader = DataLoader(datasets[\"valid\"], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_dataloader  = DataLoader(datasets[\"test\"],  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Features (log-Mel)\n",
        "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SAMPLE_RATE, n_fft=1024, hop_length=256, n_mels=80\n",
        ")\n",
        "ampl_to_db = torchaudio.transforms.AmplitudeToDB()\n",
        "\n",
        "def compute_features(sig_batch):\n",
        "    feats_list = []\n",
        "    for sig in sig_batch:\n",
        "        S = mel_spec(sig)# [n_mels, T]\n",
        "        S_db = ampl_to_db(S).transpose(0, 1)# [T, n_mels]\n",
        "        feats_list.append(S_db)\n",
        "    lens = torch.tensor([f.size(0) for f in feats_list], dtype=torch.long)\n",
        "    max_len = int(lens.max().item())\n",
        "    feat_dim = feats_list[0].size(1)\n",
        "    padded = torch.zeros(len(feats_list), max_len, feat_dim)\n",
        "    for i, f in enumerate(feats_list):\n",
        "        T = f.size(0)\n",
        "        padded[i, :T, :feat_dim] = f\n",
        "    return padded, lens\n",
        "\n",
        "# Model (CRDNN)\n",
        "class CRDNN(nn.Module):\n",
        "    def __init__(self, n_mels=80, cnn_channels=128, rnn_hidden=128, num_classes=NUM_CLASSES):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, cnn_channels, kernel_size=(5,5), stride=(1,1), padding=2),\n",
        "            nn.BatchNorm2d(cnn_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2,2)),\n",
        "            nn.Conv2d(cnn_channels, cnn_channels, kernel_size=(3,3), padding=1),\n",
        "            nn.BatchNorm2d(cnn_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2,2)),\n",
        "        )\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=(n_mels//4)*cnn_channels,\n",
        "            hidden_size=128,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128*2, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, feats, lens):\n",
        "        x = feats.unsqueeze(1)# [B, 1, T, F]\n",
        "        x = self.conv(x)# [B, C, T', F']\n",
        "        B, C, Tprime, Fprime = x.shape\n",
        "        x = x.permute(0, 2, 1, 3).contiguous().view(B, Tprime, C*Fprime)\n",
        "        out, _ = self.rnn(x)\n",
        "        out = out.mean(dim=1)# mean over time\n",
        "        return self.classifier(out)\n",
        "\n",
        "# Brain\n",
        "class SERBrain(sb.core.Brain):\n",
        "    def on_fit_start(self):\n",
        "        super().on_fit_start()\n",
        "        if \"optimizer\" not in self.checkpointer.recoverables:\n",
        "            self.checkpointer.add_recoverables({\n",
        "                \"model\": self.modules[\"model\"],\n",
        "                \"optimizer\": self.optimizer,\n",
        "                \"epoch_counter\": self.hparams.epoch_counter,\n",
        "            })\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        sigs = batch[\"sig\"]\n",
        "        labels = batch[\"label\"].to(self.device)\n",
        "        feats, lens = compute_features(sigs)\n",
        "        feats = feats.to(self.device)\n",
        "        logits = self.modules.model(feats, lens)\n",
        "        outputs = F.log_softmax(logits, dim=-1)\n",
        "        return outputs, labels\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        outputs, labels = predictions\n",
        "        loss = nll_loss(outputs, labels)\n",
        "        if stage != sb.Stage.TRAIN:  # running accuracy\n",
        "            with torch.no_grad():\n",
        "                preds = outputs.argmax(dim=-1)\n",
        "                if not hasattr(self, \"_acc_correct\"):\n",
        "                    self._acc_correct, self._acc_total = 0, 0\n",
        "                self._acc_correct += (preds == labels).sum().item()\n",
        "                self._acc_total += labels.numel()\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self._acc_correct, self._acc_total = 0, 0\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        if stage == sb.Stage.VALID:\n",
        "            acc = (self._acc_correct / max(1, self._acc_total))\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch},\n",
        "                valid_stats={\"loss\": stage_loss, \"acc\": acc},\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(meta={\"acc\": acc}, min_keys=[\"acc\"])\n",
        "        elif stage == sb.Stage.TEST:\n",
        "            acc = (self._acc_correct / max(1, self._acc_total))\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats={\"loss\": stage_loss, \"acc\": acc},\n",
        "            )\n",
        "\n",
        "# Hyperparams / objects\n",
        "run_dir = BASE / \"runs\" / \"speechbrain_ravdess\"\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "hparams = {\n",
        "    \"lr\": 1e-3,\n",
        "    \"epochs\": 25,\n",
        "    \"opt_class\": torch.optim.Adam,\n",
        "    \"model\": CRDNN(n_mels=80, cnn_channels=128, rnn_hidden=128, num_classes=NUM_CLASSES),\n",
        "    \"train_logger\": sb.utils.train_logger.FileTrainLogger(str(run_dir / \"log.txt\")),\n",
        "    \"epoch_counter\": sb.utils.epoch_loop.EpochCounter(limit=25),\n",
        "    \"checkpointer\": sb.utils.checkpoints.Checkpointer(checkpoints_dir=str(run_dir / \"ckpt\")),\n",
        "}\n",
        "brain = SERBrain(\n",
        "    modules={\"model\": hparams[\"model\"]},\n",
        "    opt_class=hparams[\"opt_class\"],\n",
        "    hparams=hparams,\n",
        "    run_opts={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
        "    checkpointer=hparams[\"checkpointer\"]\n",
        ")\n",
        "\n",
        "# Train\n",
        "brain.fit(\n",
        "    epoch_counter=brain.hparams.epoch_counter,\n",
        "    train_set=train_dataloader,\n",
        "    valid_set=valid_dataloader,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4eizvjcoWLg",
        "outputId": "98f71de9-276e-4057-e5df-a96418b66d5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.8333\n",
            "Test Macro-F1: 0.8293\n",
            "Labels: ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
            "Confusion matrix (rows=true, cols=pred):\n",
            " [[35  0  1  0  0  0  0  0]\n",
            " [ 0 34  0  0  0  0  0  0]\n",
            " [ 2  0 31  0  0  0  0  0]\n",
            " [ 0  1  2 29  1  0  2  1]\n",
            " [ 1  0  1  3 19  4  4  6]\n",
            " [ 0  0  0  0  0 18  1  0]\n",
            " [ 1  6  1  3  1  0 24  1]\n",
            " [ 2  0  0  0  0  0  0 35]]\n",
            "Saved TorchScript to: /content/drive/MyDrive/emotional_ai/runs/speechbrain_ravdess/model_best_ts.pt\n",
            "Saved: /content/drive/MyDrive/emotional_ai/models/audio/checkpoint/eval_test/metrics_overall.json\n",
            "Saved: /content/drive/MyDrive/emotional_ai/models/audio/checkpoint/eval_test/metrics_report.csv\n",
            "Saved: /content/drive/MyDrive/emotional_ai/models/audio/checkpoint/eval_test/confusion_matrix.npy\n"
          ]
        }
      ],
      "source": [
        "import os, json, numpy as np, pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Evaluate (best ckpt)\n",
        "brain.checkpointer.recover_if_possible()\n",
        "\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        outputs, labels = brain.compute_forward(batch, sb.Stage.TEST)\n",
        "        preds = outputs.argmax(dim=-1).cpu().numpy().tolist()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "all_preds_t = torch.tensor(all_preds)\n",
        "all_labels_t = torch.tensor(all_labels)\n",
        "test_acc = accuracy(all_preds_t, all_labels_t, task=\"multiclass\", num_classes=len(LABELS)).item()\n",
        "test_f1  = f1_score(all_preds_t, all_labels_t, task=\"multiclass\", num_classes=len(LABELS), average=\"macro\").item()\n",
        "cm       = confusion_matrix(all_preds_t, all_labels_t, task=\"multiclass\", num_classes=len(LABELS)).cpu().numpy()\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test Macro-F1: {test_f1:.4f}\")\n",
        "print(\"Labels:\", LABELS)\n",
        "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
        "\n",
        "# Save artifacts\n",
        "np.save(run_dir / \"confusion_matrix.npy\", cm)\n",
        "with open(run_dir / \"labels.json\", \"w\") as f:\n",
        "    json.dump(LABELS, f, indent=2)\n",
        "torch.save(brain.modules[\"model\"].state_dict(), run_dir / \"model_best_state.pt\")\n",
        "\n",
        "# TorchScript export (trace - fallback to script)\n",
        "m = brain.modules[\"model\"].eval()\n",
        "device = next(m.parameters()).device\n",
        "ex_T = 400\n",
        "example_feats = torch.randn(1, ex_T, 80, device=device, dtype=torch.float32).contiguous()\n",
        "example_lens  = torch.tensor([ex_T], device=device, dtype=torch.long)\n",
        "\n",
        "with torch.inference_mode():\n",
        "    try:\n",
        "        ts = torch.jit.trace(m, (example_feats, example_lens), strict=False)\n",
        "        _ = ts(example_feats, example_lens)\n",
        "    except Exception:\n",
        "        class Wrapper(torch.nn.Module):\n",
        "            def __init__(self, core): super().__init__(); self.core = core\n",
        "            def forward(self, feats: torch.Tensor, lens: torch.Tensor): return self.core(feats, lens)\n",
        "        ts = torch.jit.script(Wrapper(m).to(device).eval())\n",
        "\n",
        "ts = ts.to(\"cpu\")\n",
        "ts_path = run_dir / \"model_best_ts.pt\"\n",
        "ts.save(str(ts_path))\n",
        "print(\"Saved TorchScript to:\", ts_path)\n",
        "\n",
        "\n",
        "# FORMAL TEST ARTIFACTS (overall and per-class)\n",
        "# Where to store\n",
        "MODEL_DIR = BASE / \"models\" / \"audio\" / \"checkpoint\"\n",
        "EVAL_DIR  = MODEL_DIR / \"eval_test\"\n",
        "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Overall summary\n",
        "overall = {\n",
        "    \"accuracy\": float(test_acc),\n",
        "    \"macro/f1\": float(test_f1),\n",
        "    \"num_classes\": int(len(LABELS)),\n",
        "    \"num_samples\": int(len(all_labels)),\n",
        "}\n",
        "\n",
        "# Per-class table\n",
        "rep = classification_report(all_labels, all_preds,\n",
        "                            target_names=LABELS,\n",
        "                            output_dict=True,\n",
        "                            zero_division=0)\n",
        "rep_df = pd.DataFrame(rep).transpose()  # includes each class and macro/weighted avg\n",
        "\n",
        "# Save files\n",
        "with open(EVAL_DIR / \"metrics_overall.json\", \"w\") as f:\n",
        "    json.dump(overall, f, indent=2)\n",
        "\n",
        "rep_df.to_csv(EVAL_DIR / \"metrics_report.csv\")# per-class P/R/F1 and support\n",
        "\n",
        "# Move/copy confusion matrix into eval_test (if it’s elsewhere)\n",
        "src_cm = run_dir / \"confusion_matrix.npy\"\n",
        "dst_cm = EVAL_DIR / \"confusion_matrix.npy\"\n",
        "if src_cm.exists() and str(src_cm) != str(dst_cm):\n",
        "    import shutil; shutil.copy2(src_cm, dst_cm)\n",
        "\n",
        "print(\"Saved:\", EVAL_DIR / \"metrics_overall.json\")\n",
        "print(\"Saved:\", EVAL_DIR / \"metrics_report.csv\")\n",
        "print(\"Saved:\", dst_cm)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
