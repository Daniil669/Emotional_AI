{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMsnDt__0Yzv",
        "outputId": "9f5cb6a2-9e59-45d2-df36-dd2132d41b7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "328HiZhAdqvi",
        "outputId": "d8b14caa-ebaf-46aa-c7d7-c6590fb1b1a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "timm 1.0.19 requires torchvision, which is not installed.\n",
            "fastai 2.8.4 requires torchvision>=0.11, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Clean, compatible stack for SpeechBrain on Colab\n",
        "!pip -q install -U pip\n",
        "\n",
        "# 1 Remove the conflicting torchvision\n",
        "!pip -q uninstall -y torchvision\n",
        "\n",
        "# 2 Install a matched torch/torchaudio pair (2.3.0)\n",
        "!pip -q install torch==2.3.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "# 3 Install the rest without downgrading numpy/sklearn\n",
        "!pip -q install speechbrain==0.5.16 torchmetrics>=1.3 librosa>=0.10 pandas>=2.2 scikit-learn>=1.5 numpy>=2.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7W5nyshyfZBV",
        "outputId": "1dadc2a2-331e-48c4-83e7-fde848e33b15"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:speechbrain.utils.train_logger:torchvision is not available - cannot save figures\n",
            "100%|██████████| 87/87 [24:23<00:00, 16.82s/it, train_loss=1.93]\n",
            "100%|██████████| 18/18 [04:03<00:00, 13.55s/it]\n",
            "100%|██████████| 87/87 [04:30<00:00,  3.11s/it, train_loss=1.81]\n",
            "100%|██████████| 18/18 [00:25<00:00,  1.44s/it]\n",
            "100%|██████████| 87/87 [04:33<00:00,  3.15s/it, train_loss=1.78]\n",
            "100%|██████████| 18/18 [00:27<00:00,  1.52s/it]\n",
            "100%|██████████| 87/87 [04:30<00:00,  3.11s/it, train_loss=1.76]\n",
            "100%|██████████| 18/18 [00:27<00:00,  1.53s/it]\n",
            "100%|██████████| 87/87 [04:26<00:00,  3.06s/it, train_loss=1.72]\n",
            "100%|██████████| 18/18 [00:27<00:00,  1.50s/it]\n",
            "100%|██████████| 87/87 [04:28<00:00,  3.09s/it, train_loss=1.69]\n",
            "100%|██████████| 18/18 [00:26<00:00,  1.46s/it]\n",
            "100%|██████████| 87/87 [04:31<00:00,  3.12s/it, train_loss=1.71]\n",
            "100%|██████████| 18/18 [00:26<00:00,  1.50s/it]\n",
            "100%|██████████| 87/87 [04:33<00:00,  3.14s/it, train_loss=1.63]\n",
            "100%|██████████| 18/18 [00:26<00:00,  1.45s/it]\n",
            "100%|██████████| 87/87 [04:30<00:00,  3.11s/it, train_loss=1.61]\n",
            "100%|██████████| 18/18 [00:27<00:00,  1.52s/it]\n",
            "100%|██████████| 87/87 [04:33<00:00,  3.14s/it, train_loss=1.6]\n",
            "100%|██████████| 18/18 [00:26<00:00,  1.50s/it]\n",
            "100%|██████████| 87/87 [04:32<00:00,  3.13s/it, train_loss=1.54]\n",
            "100%|██████████| 18/18 [00:26<00:00,  1.49s/it]\n",
            "100%|██████████| 87/87 [04:29<00:00,  3.09s/it, train_loss=1.54]\n",
            "100%|██████████| 18/18 [00:27<00:00,  1.50s/it]\n",
            "100%|██████████| 87/87 [04:31<00:00,  3.12s/it, train_loss=1.47]\n",
            "100%|██████████| 18/18 [00:27<00:00,  1.55s/it]\n",
            "100%|██████████| 87/87 [04:35<00:00,  3.17s/it, train_loss=1.47]\n",
            "100%|██████████| 18/18 [00:27<00:00,  1.54s/it]\n",
            "100%|██████████| 87/87 [04:40<00:00,  3.23s/it, train_loss=1.44]\n",
            "100%|██████████| 18/18 [00:29<00:00,  1.62s/it]\n",
            "100%|██████████| 87/87 [04:39<00:00,  3.22s/it, train_loss=1.32]\n",
            "100%|██████████| 18/18 [00:26<00:00,  1.49s/it]\n",
            "100%|██████████| 87/87 [04:37<00:00,  3.19s/it, train_loss=1.29]\n",
            "100%|██████████| 18/18 [00:27<00:00,  1.54s/it]\n",
            "100%|██████████| 87/87 [04:33<00:00,  3.14s/it, train_loss=1.22]\n",
            "100%|██████████| 18/18 [00:27<00:00,  1.52s/it]\n",
            "100%|██████████| 87/87 [04:36<00:00,  3.17s/it, train_loss=1.19]\n",
            "100%|██████████| 18/18 [00:27<00:00,  1.52s/it]\n",
            "100%|██████████| 87/87 [04:33<00:00,  3.14s/it, train_loss=1.16]\n",
            "100%|██████████| 18/18 [00:27<00:00,  1.52s/it]\n",
            "100%|██████████| 87/87 [04:37<00:00,  3.19s/it, train_loss=1.12]\n",
            "100%|██████████| 18/18 [00:27<00:00,  1.53s/it]\n",
            "100%|██████████| 87/87 [04:34<00:00,  3.15s/it, train_loss=1.05]\n",
            "100%|██████████| 18/18 [00:28<00:00,  1.56s/it]\n",
            "100%|██████████| 87/87 [04:40<00:00,  3.23s/it, train_loss=1.08]\n",
            "100%|██████████| 18/18 [00:26<00:00,  1.50s/it]\n",
            "100%|██████████| 87/87 [04:36<00:00,  3.18s/it, train_loss=1.02]\n",
            "100%|██████████| 18/18 [00:27<00:00,  1.55s/it]\n",
            "100%|██████████| 87/87 [04:40<00:00,  3.22s/it, train_loss=0.96]\n",
            "100%|██████████| 18/18 [00:28<00:00,  1.56s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.6778\n",
            "Test Macro-F1: 0.6784\n",
            "Labels: ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
            "Confusion matrix (rows=true, cols=pred):\n",
            " [[30  0  4  1  0  0  0  1]\n",
            " [ 0 29  0  0  1  1  3  0]\n",
            " [ 7  1 22  1  0  0  2  0]\n",
            " [ 0  1  0 23  5  0  6  1]\n",
            " [ 0  0  0  3 22  2  4  7]\n",
            " [ 0  2  0  0  1 15  1  0]\n",
            " [ 0 13  0  5  5  1 10  3]\n",
            " [ 1  0  0  0  3  0  1 32]]\n",
            "Saved: /content/drive/MyDrive/emotional_ai/runs/speechbrain_ravdess/model_best_ts.pt\n",
            "Labels: ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n"
          ]
        }
      ],
      "source": [
        "# Clean, runnable SER training cell (SpeechBrain + torchaudio)\n",
        "import json\n",
        "from pathlib import Path\n",
        "import torch, torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import speechbrain as sb\n",
        "from speechbrain.nnet.losses import nll_loss\n",
        "from torchmetrics.functional import f1_score, confusion_matrix, accuracy\n",
        "from torch.utils.data import DataLoader\n",
        "from speechbrain.dataio.dataset import DynamicItemDataset\n",
        "\n",
        "\n",
        "# Paths / labels\n",
        "BASE = Path(\"/content/drive/MyDrive/emotional_ai\")\n",
        "DATA_DIR = BASE / \"data\" / \"audio\" / \"ravdess\"\n",
        "AUDIO_ROOT = DATA_DIR / \"wav\"\n",
        "META_DIR = DATA_DIR / \"meta\"\n",
        "\n",
        "with open(META_DIR / \"labels.json\") as f:\n",
        "    LABELS = json.load(f)\n",
        "NUM_CLASSES = len(LABELS)\n",
        "label_to_index = {lab: i for i, lab in enumerate(LABELS)}\n",
        "\n",
        "train_csv = META_DIR / \"train.csv\"\n",
        "valid_csv = META_DIR / \"valid.csv\"\n",
        "test_csv  = META_DIR / \"test.csv\"\n",
        "assert train_csv.exists() and valid_csv.exists() and test_csv.exists(), \"Missing CSVs!\"\n",
        "\n",
        "# Dataset helpers\n",
        "SAMPLE_RATE = 16000\n",
        "\n",
        "def read_csv_to_items(path: Path):\n",
        "    df = pd.read_csv(path)  # expects: ID,wav,duration,emotion\n",
        "    df = df[df[\"emotion\"].isin(LABELS)]\n",
        "    items = []\n",
        "    for _, r in df.iterrows():\n",
        "        items.append({\n",
        "            \"id\": str(r[\"ID\"]),\n",
        "            \"wav\": str(r[\"wav\"]),\n",
        "            \"emotion\": label_to_index[str(r[\"emotion\"])],\n",
        "        })\n",
        "    return items\n",
        "\n",
        "def items_to_mapping(items):\n",
        "    # DynamicItemDataset expects dict keyed by \"id\"\n",
        "    return {it[\"id\"]: {\"wav\": it[\"wav\"], \"emotion\": it[\"emotion\"]} for it in items}\n",
        "\n",
        "train_items = read_csv_to_items(train_csv)\n",
        "valid_items = read_csv_to_items(valid_csv)\n",
        "test_items  = read_csv_to_items(test_csv)\n",
        "\n",
        "datasets = {\n",
        "    \"train\": DynamicItemDataset(items_to_mapping(train_items)),\n",
        "    \"valid\": DynamicItemDataset(items_to_mapping(valid_items)),\n",
        "    \"test\":  DynamicItemDataset(items_to_mapping(test_items)),\n",
        "}\n",
        "\n",
        "\n",
        "# Pipelines (audio + labels)\n",
        "def audio_pipeline(wav_path):\n",
        "    sig, sr = torchaudio.load(wav_path)# [C, T]\n",
        "    if sig.shape[0] > 1:\n",
        "        sig = sig.mean(dim=0, keepdim=True) # mono\n",
        "    if sr != SAMPLE_RATE:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=SAMPLE_RATE)\n",
        "        sig = resampler(sig)\n",
        "    return sig.squeeze(0)# [T]\n",
        "\n",
        "def label_pipeline(idx):\n",
        "    return torch.tensor(idx, dtype=torch.long)\n",
        "\n",
        "for split in datasets:\n",
        "    datasets[split].add_dynamic_item(audio_pipeline, takes=\"wav\", provides=\"sig\")\n",
        "    datasets[split].add_dynamic_item(label_pipeline,  takes=\"emotion\", provides=\"label\")\n",
        "    datasets[split].set_output_keys([\"id\", \"sig\", \"label\"])\n",
        "\n",
        "\n",
        "# Dataloaders (custom collate)\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Keep variable-length signals as a list, stack labels into a tensor\n",
        "    sigs = [b[\"sig\"] for b in batch]  # list of 1D tensors\n",
        "    labels = torch.tensor([int(b[\"label\"].item()) for b in batch], dtype=torch.long)\n",
        "    return {\"sig\": sigs, \"label\": labels}\n",
        "\n",
        "train_dataloader = DataLoader(datasets[\"train\"], batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn)\n",
        "valid_dataloader = DataLoader(datasets[\"valid\"], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_dataloader  = DataLoader(datasets[\"test\"],  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "# Features (log-Mel)\n",
        "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SAMPLE_RATE, n_fft=1024, hop_length=256, n_mels=80\n",
        ")\n",
        "ampl_to_db = torchaudio.transforms.AmplitudeToDB()\n",
        "\n",
        "def compute_features(sig_batch):\n",
        "    feats_list = []\n",
        "    for sig in sig_batch:# sig: 1D\n",
        "        S = mel_spec(sig)# [n_mels, T]\n",
        "        S_db = ampl_to_db(S).transpose(0, 1)# [T, n_mels]\n",
        "        feats_list.append(S_db)\n",
        "    lens = torch.tensor([f.size(0) for f in feats_list], dtype=torch.long)\n",
        "    max_len = int(lens.max().item())\n",
        "    feat_dim = feats_list[0].size(1)\n",
        "    padded = torch.zeros(len(feats_list), max_len, feat_dim)\n",
        "    for i, f in enumerate(feats_list):\n",
        "        T = f.size(0)\n",
        "        padded[i, :T, :feat_dim] = f\n",
        "    return padded, lens\n",
        "\n",
        "# Model (CRDNN)\n",
        "class CRDNN(nn.Module):\n",
        "    def __init__(self, n_mels=80, cnn_channels=128, rnn_hidden=128, num_classes=NUM_CLASSES):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, cnn_channels, kernel_size=(5,5), stride=(1,1), padding=2),\n",
        "            nn.BatchNorm2d(cnn_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2,2)),\n",
        "            nn.Conv2d(cnn_channels, cnn_channels, kernel_size=(3,3), padding=1),\n",
        "            nn.BatchNorm2d(cnn_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2,2)),\n",
        "        )\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=(n_mels//4)*cnn_channels,\n",
        "            hidden_size=rnn_hidden,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_hidden*2, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, feats, lens):\n",
        "        # feats: [B, T, F] >> [B, 1, T, F]\n",
        "        x = feats.unsqueeze(1)\n",
        "        x = self.conv(x)# [B, C, T', F']\n",
        "        B, C, Tprime, Fprime = x.shape\n",
        "        x = x.permute(0, 2, 1, 3).contiguous().view(B, Tprime, C*Fprime)\n",
        "        out, _ = self.rnn(x)\n",
        "        out = out.mean(dim=1)# mean pool over time\n",
        "        logits = self.classifier(out)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Brain (with built-in accuracy counter)\n",
        "class SERBrain(sb.core.Brain):\n",
        "    def on_fit_start(self):\n",
        "        super().on_fit_start()\n",
        "        if \"optimizer\" not in self.checkpointer.recoverables:\n",
        "            self.checkpointer.add_recoverables({\n",
        "                \"model\": self.modules[\"model\"],\n",
        "                \"optimizer\": self.optimizer,\n",
        "                \"epoch_counter\": self.hparams.epoch_counter,\n",
        "            })\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        # batch is dict from the collate_fn\n",
        "        sigs = batch[\"sig\"]# list[Tensor(T_i)]\n",
        "        labels = batch[\"label\"].to(self.device)# [B]\n",
        "        feats, lens = compute_features(sigs)# >> [B, Tmax, F], [B]\n",
        "        feats = feats.to(self.device)\n",
        "        logits = self.modules.model(feats, lens)# [B, C]\n",
        "        outputs = F.log_softmax(logits, dim=-1)\n",
        "        return outputs, labels\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        outputs, labels = predictions\n",
        "        loss = nll_loss(outputs, labels)\n",
        "\n",
        "        # Running accuracy for VALID/TEST (no external imports)\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            with torch.no_grad():\n",
        "                preds = outputs.argmax(dim=-1)\n",
        "                if not hasattr(self, \"_acc_correct\"):\n",
        "                    self._acc_correct, self._acc_total = 0, 0\n",
        "                self._acc_correct += (preds == labels).sum().item()\n",
        "                self._acc_total += labels.numel()\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self._acc_correct, self._acc_total = 0, 0\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        if stage == sb.Stage.VALID:\n",
        "            acc = (self._acc_correct / max(1, self._acc_total))\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch},\n",
        "                valid_stats={\"loss\": stage_loss, \"acc\": acc},\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(meta={\"acc\": acc}, min_keys=[\"acc\"])\n",
        "        elif stage == sb.Stage.TEST:\n",
        "            acc = (self._acc_correct / max(1, self._acc_total))\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats={\"loss\": stage_loss, \"acc\": acc},\n",
        "            )\n",
        "\n",
        "\n",
        "# Hyperparams / objects\n",
        "run_dir = BASE / \"runs\" / \"speechbrain_ravdess\"\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "hparams = {\n",
        "    \"lr\": 1e-3,\n",
        "    \"epochs\": 25,\n",
        "    \"opt_class\": torch.optim.Adam,\n",
        "    \"model\": CRDNN(n_mels=80, cnn_channels=128, rnn_hidden=128, num_classes=NUM_CLASSES),\n",
        "    \"train_logger\": sb.utils.train_logger.FileTrainLogger(str(run_dir / \"log.txt\")),\n",
        "    \"epoch_counter\": sb.utils.epoch_loop.EpochCounter(limit=25),\n",
        "    \"checkpointer\": sb.utils.checkpoints.Checkpointer(checkpoints_dir=str(run_dir / \"ckpt\")),\n",
        "}\n",
        "brain = SERBrain(\n",
        "    modules={\"model\": hparams[\"model\"]},\n",
        "    opt_class=hparams[\"opt_class\"],\n",
        "    hparams=hparams,\n",
        "    run_opts={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
        "    checkpointer=hparams[\"checkpointer\"]\n",
        ")\n",
        "\n",
        "\n",
        "# Train\n",
        "brain.fit(\n",
        "    epoch_counter=brain.hparams.epoch_counter,\n",
        "    train_set=train_dataloader,\n",
        "    valid_set=valid_dataloader,\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate (best ckpt) + metrics\n",
        "brain.checkpointer.recover_if_possible()\n",
        "\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        outputs, labels = brain.compute_forward(batch, sb.Stage.TEST)\n",
        "        _ = brain.compute_objectives((outputs, labels), batch, sb.Stage.TEST)\n",
        "        preds = outputs.argmax(dim=-1).cpu().numpy().tolist()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "all_preds_t = torch.tensor(all_preds)\n",
        "all_labels_t = torch.tensor(all_labels)\n",
        "test_acc = accuracy(all_preds_t, all_labels_t, task=\"multiclass\", num_classes=NUM_CLASSES).item()\n",
        "test_f1  = f1_score(all_preds_t, all_labels_t, task=\"multiclass\", num_classes=NUM_CLASSES, average=\"macro\").item()\n",
        "cm       = confusion_matrix(all_preds_t, all_labels_t, task=\"multiclass\", num_classes=NUM_CLASSES).cpu().numpy()\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test Macro-F1: {test_f1:.4f}\")\n",
        "print(\"Labels:\", LABELS)\n",
        "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
        "\n",
        "# Save artifacts\n",
        "np.save(run_dir / \"confusion_matrix.npy\", cm)\n",
        "with open(run_dir / \"labels.json\", \"w\") as f:\n",
        "    json.dump(LABELS, f, indent=2)\n",
        "torch.save(brain.modules[\"model\"].state_dict(), run_dir / \"model_best_state.pt\")\n",
        "\n",
        "# Robust TorchScript export (trace - fallback to script)\n",
        "m = brain.modules[\"model\"].eval()\n",
        "\n",
        "# Put example tensors on the SAME device and dtype as the model\n",
        "device = next(m.parameters()).device\n",
        "ex_T = 400\n",
        "example_feats = torch.randn(1, ex_T, 80, device=device, dtype=torch.float32).contiguous()\n",
        "example_lens  = torch.tensor([ex_T], device=device, dtype=torch.long)\n",
        "\n",
        "with torch.inference_mode():\n",
        "    try:\n",
        "        # Try tracing first (fast, portable) strict=False tolerates control-flow it can’t follow.\n",
        "        ts = torch.jit.trace(m, (example_feats, example_lens), strict=False)\n",
        "        _ = ts(example_feats, example_lens)  # sanity run\n",
        "    except Exception as e:\n",
        "        # Fallback to scripting (handles more dynamic bits)\n",
        "        class Wrapper(torch.nn.Module):\n",
        "            def __init__(self, core):\n",
        "                super().__init__()\n",
        "                self.core = core\n",
        "            def forward(self, feats: torch.Tensor, lens: torch.Tensor):\n",
        "                return self.core(feats, lens)\n",
        "        wrapped = Wrapper(m).to(device).eval()\n",
        "        ts = torch.jit.script(wrapped)\n",
        "\n",
        "# Save TorchScript on CPU for portability\n",
        "ts = ts.to(\"cpu\")\n",
        "ts_path = run_dir / \"model_best_ts.pt\"\n",
        "ts.save(str(ts_path))\n",
        "\n",
        "# Save labels next to the model\n",
        "with open(run_dir / \"labels.json\", \"w\") as f:\n",
        "    json.dump(LABELS, f, indent=2)\n",
        "\n",
        "print(\"Saved:\", ts_path)\n",
        "print(\"Labels:\", LABELS)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOGICveiC3xI",
        "outputId": "c8d973c9-b170-4608-beda-54faf960082a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exported SpeechBrain savedir to: /content/drive/MyDrive/emotional_ai/emotional_ai_export/audio/checkpoint\n"
          ]
        }
      ],
      "source": [
        "import os, json, shutil\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "SAVE_ROOT = \"/content/drive/MyDrive/emotional_ai/emotional_ai_export/audio\"\n",
        "SB_SRC    = \"/content/drive/MyDrive/emotional_ai/runs/speechbrain_ravdess\" #check out\n",
        "SB_DST    = f\"{SAVE_ROOT}/checkpoint\"\n",
        "\n",
        "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
        "shutil.copytree(SB_SRC, SB_DST, dirs_exist_ok=True)\n",
        "\n",
        "labels = [\"neutral\",\"calm\",\"happy\",\"sad\",\"angry\",\"fearful\",\"disgust\",\"surprised\"]\n",
        "with open(f\"{SAVE_ROOT}/labels.json\", \"w\") as f:\n",
        "    json.dump(labels, f)\n",
        "\n",
        "with open(f\"{SAVE_ROOT}/model_meta.json\", \"w\") as f:\n",
        "    json.dump({\"name\":\"speechbrain-ser\",\n",
        "               \"version\": datetime.now(timezone.utc).strftime(\"%Y%m%d\")}, f)\n",
        "\n",
        "print(\"Exported SpeechBrain savedir to:\", SB_DST)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
